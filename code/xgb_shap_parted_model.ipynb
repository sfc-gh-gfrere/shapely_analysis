{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "0776bc94-e2b7-4039-9a83-370d86fa38e7",
   "metadata": {
    "language": "python",
    "name": "cell6",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "####################################################################################\n# Weather Prediction Using DFT, XGBoost, and SHAP                                 #\n####################################################################################\n\n# Author: Garrett Frere\n\n# Description:\n# Predicts hourly temperatures using XGBoost and interprets predictions with SHAP. \n# Normalizes temperature via DFT and processes data partitioned by `LOCATION`.\n\n# Key Features:\n# 1. **Time-Series Preprocessing**:\n#    - Applies DFT to `TEMP` for frequency-domain normalization.\n\n# 2. **Model Training and Prediction**:\n#    - Trains XGBoost to predict `TEMP` using features like `TEMP_FFT` and `HOUR_OF_DAY`.\n\n# 3. **SHAP Analysis**:\n#    - Computes global SHAP values for feature importance.\n#    - Supports local SHAP analysis per partition (e.g., `LOCATION`).\n\n# Use Cases:\n# - Forecasting hourly temperatures.\n# - Understanding feature influence using SHAP.\n\n# Requirements:\n# - Python 3.8+ (3.11 preferred for performance), pandas, numpy, scipy, xgboost, shap.\n\n####################################################################################",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "resultHeight": 0,
    "collapsed": false
   },
   "source": "# Snowpark for Python\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.version import VERSION\nfrom snowflake.snowpark.functions import udtf\nfrom snowflake.snowpark.context import get_active_session\nimport snowflake.snowpark.functions as F\n\n# Snowpark ML\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.model import custom_model\nfrom snowflake.ml.model import model_signature\n\n# data science libs\nfrom scipy.fft import fft\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport shap\nimport matplotlib.pyplot as plt\n\n# misc\nimport json\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "062a9902-59ab-47af-a4da-dec7308f73ef",
   "metadata": {
    "language": "python",
    "name": "cell4",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Resize output width\npd.set_option('display.width', 1000)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "77c24e63-4c67-4401-b85e-886614f09654",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "resultHeight": 739
   },
   "outputs": [],
   "source": "def generate_temperature_data(start_date, num_days, locations):\n    \"\"\"\n    Generates synthetic weather data for multiple locations, including temperature \n    and an unrelated feature for demonstration purposes.\n    \n    Args:\n        start_date (str): The starting date in 'YYYY-MM-DD' format.\n        num_days (int): Number of days to generate data for.\n        locations (list): List of location names.\n\n    Returns:\n        pd.DataFrame: Synthetic weather data.\n    \"\"\"\n    # Generate hourly timestamps\n    hours_per_day = 24\n    total_hours = num_days * hours_per_day\n    date_range = pd.date_range(start=start_date, periods=total_hours, freq='h')\n\n    # Initialize synthetic data\n    data = []\n\n    for location in locations:\n        for hour in range(total_hours):\n            # Generate temperature with daily sinusoidal pattern\n            time_of_day = hour % 24\n            base_temp = 15 + 10 * np.sin(2 * np.pi * time_of_day / 24)  # Sinusoidal pattern\n            temp_variation = np.random.normal(scale=2)  # Add random noise\n            temp = base_temp + temp_variation\n\n            # Add random unrelated feature\n            random_noise = np.random.uniform(0, 100)  # Unrelated random values\n\n            data.append({\n                \"DATE\": date_range[hour],\n                \"TEMP\": temp,\n                \"LOCATION\": location,\n                \"HOUR_OF_DAY\": time_of_day,\n                \"RANDOM_NOISE\": random_noise\n            })\n\n    return pd.DataFrame(data)\n\n# Generate data for multiple locations\nlocations = ['New York', 'Los Angeles', 'Chicago']\ndf = generate_temperature_data(start_date='2023-01-01', num_days=3, locations=locations)\n\n# Plot temperature data for all locations on a single plot\nplt.figure(figsize=(12, 6))\nfor location in locations:\n    location_data = df[df['LOCATION'] == location]\n    plt.plot(\n        location_data['DATE'], \n        location_data['TEMP'], \n        label=location\n    )\n\n# Customize the plot\nplt.title('Simulated Temperature Data for Multiple Locations')\nplt.xlabel('Time')\nplt.ylabel('Temperature (Â°C)')\nplt.legend(title='Location')\nplt.grid()\nplt.xticks(rotation=45)\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a93bf009-26b2-48f3-94da-1fea14da0ef9",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "resultHeight": 643,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Generate data for multiple locations\nlocations = ['New York', 'Los Angeles', 'Chicago', 'Tokyo']\ndf = generate_temperature_data(start_date='2023-01-01', num_days=5, locations=locations)\n\nfor location in locations:\n    print(f\"Head for {location}:\")\n    print(df[df['LOCATION'] == location].head(), \"\\n\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e074670-01eb-4751-9b6e-0146994759f9",
   "metadata": {
    "language": "python",
    "name": "cell9",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "class WeatherPredictor:\n    \"\"\"Weather Prediction using XGBoost with hourly features and SHAP.\"\"\"\n\n    def prepare_data(self, input: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Normalizes weather data using FFT, excluding the DC component.\n        \n        Args:\n            input (pd.DataFrame): Raw DataFrame containing 'TEMP' and 'HOUR_OF_DAY'.\n        \n        Returns:\n            pd.DataFrame: DataFrame with an added 'TEMP_FFT' column.\n        \"\"\"\n        # Validate required columns\n        required_columns = {'TEMP', 'HOUR_OF_DAY'}\n        if not required_columns.issubset(input.columns):\n            missing = required_columns - set(input.columns)\n            raise KeyError(f\"Missing required columns: {missing}\")\n\n        # Ensure 'TEMP' is numeric\n        if not pd.api.types.is_numeric_dtype(input['TEMP']):\n            raise ValueError(\"'TEMP' column must be numeric.\")\n\n        # Perform FFT and exclude DC component\n        input = input.copy()\n        temp_array = input['TEMP'].to_numpy()\n        fft_values = fft(temp_array)\n        input = input.iloc[1:].copy()  # Drop the first row\n        input['TEMP_FFT'] = np.abs(fft_values[1:])  # Exclude the first value (DC component)\n\n        return input\n        \n    @custom_model.partitioned_inference_api\n    def predict(self, input: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Prepares data using FFT and trains an XGBoost model to predict temperature.\n        \n        Args:\n            input (pd.DataFrame): Raw DataFrame with 'TEMP' and 'HOUR_OF_DAY'.\n        \n        Returns:\n            pd.DataFrame: DataFrame with predictions added as a 'PREDICTED_TEMP' column.\n        \"\"\"\n        # Prepare data with FFT normalization\n        prepared_data = self.prepare_data(input)\n\n        # Extract features (X) and target (y)\n        X = prepared_data.drop(columns=['TEMP', 'DATE'])  # Drop non-feature columns\n        y = prepared_data['TEMP']\n        \n        # Convert LOCATION to categorical dtype\n        if 'LOCATION' in X.columns:\n            X['LOCATION'] = X['LOCATION'].astype('category')\n\n        # Train the XGBoost model\n        xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, enable_categorical=True)\n        xgb_model.fit(X, y)\n\n        # Make predictions\n        prepared_data['PREDICTED_TEMP'] = xgb_model.predict(X)\n\n        # Save model and features for further use (e.g., SHAP)\n        self.xgb_model = xgb_model\n        self.X = X\n\n        return prepared_data[['DATE', 'LOCATION', 'HOUR_OF_DAY', 'TEMP', 'PREDICTED_TEMP']]\n\n    def calculate_global_shap(self) -> pd.DataFrame:\n        \"\"\"Calculates global SHAP values for the trained XGBoost model.\"\"\"\n        if not hasattr(self, 'xgb_model'):\n            raise ValueError(\"Model not trained. Run 'predict' first.\")\n\n        # Calculate SHAP values\n        explainer = shap.Explainer(self.xgb_model)\n        shap_values = explainer(self.X)\n\n        # Aggregate global SHAP values\n        global_shap = np.abs(shap_values.values).mean(axis=0)\n        feature_importance = pd.DataFrame({\n            'FEATURE': self.X.columns,\n            'GLOBAL_SHAP': global_shap\n        }).sort_values(by='GLOBAL_SHAP', ascending=False)\n\n        return feature_importance",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "12d975d9-4211-4f5d-adec-cf4bfa748c4d",
   "metadata": {
    "language": "python",
    "name": "cell8",
    "resultHeight": 307
   },
   "outputs": [],
   "source": "# Generate synthetic data\nlocations = ['New York', 'Los Angeles', 'Chicago', 'Tokyo']\nsynthetic_data = generate_temperature_data(start_date='2023-01-01', num_days=3, locations=locations)\n\n# Filter data for one location\nny_data = synthetic_data[synthetic_data['LOCATION'] == 'New York']\n\n# Instantiate and run the predictor\npredictor = WeatherPredictor()\npredictions = predictor.predict(ny_data)\n\n# Display predictions\nprint(\"Predictions:\")\nprint(predictions.head())\n\n# Calculate global SHAP values\nglobal_shap = predictor.calculate_global_shap()\nprint(\"\\nGlobal SHAP Values:\")\nprint(global_shap)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4de0e35-f765-41a7-bd03-94dd37118681",
   "metadata": {
    "language": "python",
    "name": "cell10",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "table_name = \"synth_weather_data\"\n\nsession.use_database('GFRERE_DB')\nsession.use_schema('SHAP')\n\n# Write Pandas DataFrame to Snowflake\nsnow_synth_weather_df = session.write_pandas(\n    synthetic_data,\n    table_name,\n    auto_create_table=True,\n    overwrite=True,  # Added missing comma\n    # schema=schema\n)\n\n# snow_synth_weather_df.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "13c83b34-433c-4b7b-97e6-7f61cbb00b3d",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "# Create a model registry connection using the Snowpark session object, we will use the current database and schema for storing the model.\nsnowml_registry = Registry(session)\n\n# Infer input-output schema for model tracking\npredict_signature = model_signature.infer_signature(input_data=snow_synth_weather_df, output_data=predictions)\n\ncustom_mv = snowml_registry.log_model(\n    my_sim,\n    model_name=\"WeatherPredictor\",\n    version_name=\"version\",\n    conda_dependencies=[\"pandas==2.2.2\", \"numpy==1.26.4\"], #TODO add from scipy.fft import fft, XGBOOST, SHAP\n    signatures={\"predict\": predict_signature},\n    comment = 'Testing SHAP on location partions for XGB',\n    options={\"relax_version\": True, \"function_type\": \"TABLE_FUNCTION\"}\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "91cac983-3bbc-4f70-bd53-bd873bf86b9c",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "# Partitioned Custom Model can be invoked with a partition_column arg\n# Runs the registered model\n# You need to run the synthetic weather data generator and\n# Save it to a view and create a Snow DF called 'snow_raw_df'\nouptut_snow_df = custom_mv.run(\n  snow_raw_df,\n  function_name=\"PREDICT\",\n  partition_column=\"LOCATION\"\n)\n\nouptut_snow_df.show()\n# session.write(ouptut_snow_df, 'incident_tbl')",
   "execution_count": null
  }
 ]
}